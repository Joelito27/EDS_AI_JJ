{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c04f677-a464-40ae-92bf-aa789dc176d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-4.9.9-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.12/site-packages (from tensorflow_datasets) (2.2.2)\n",
      "Collecting array_record>=0.5.0 (from tensorflow_datasets)\n",
      "  Using cached array_record-0.8.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Using cached dm_tree-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Using cached etils-1.13.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Using cached immutabledict-4.2.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.12/site-packages (from tensorflow_datasets) (4.25.6)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from tensorflow_datasets) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.12/site-packages (from tensorflow_datasets) (19.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Collecting simple_parsing (from tensorflow_datasets)\n",
      "  Using cached simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Using cached tensorflow_metadata-1.17.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.12/site-packages (from tensorflow_datasets) (3.0.1)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from tensorflow_datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from tensorflow_datasets) (1.17.2)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2025.3.2)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.13.2)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.21.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow_datasets) (2025.1.31)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /opt/conda/lib/python3.12/site-packages (from dm-tree->tensorflow_datasets) (25.3.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.12/site-packages (from promise->tensorflow_datasets) (1.17.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow_datasets)\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Using cached tensorflow_datasets-4.9.9-py3-none-any.whl (5.3 MB)\n",
      "Using cached array_record-0.8.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "Using cached etils-1.13.0-py3-none-any.whl (170 kB)\n",
      "Using cached dm_tree-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "Using cached immutabledict-4.2.2-py3-none-any.whl (4.7 kB)\n",
      "Using cached simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Using cached tensorflow_metadata-1.17.2-py3-none-any.whl (31 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Using cached googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: toml, promise, immutabledict, googleapis-common-protos, etils, einops, docstring-parser, dm-tree, tensorflow-metadata, simple_parsing, array_record, tensorflow_datasets\n",
      "Successfully installed array_record-0.8.3 dm-tree-0.1.9 docstring-parser-0.17.0 einops-0.8.1 etils-1.13.0 googleapis-common-protos-1.72.0 immutabledict-4.2.2 promise-2.3 simple_parsing-0.1.7 tensorflow-metadata-1.17.2 tensorflow_datasets-4.9.9 toml-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c7e4a12-508d-4def-b9f7-4a65b01172c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder /home/jovyan/tensorflow_datasets/gtzan/1.0.0 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/jovyan/tensorflow_datasets/gtzan/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6730c76dcb7c4fa78d84cfc0e913021e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f4108d8aa049e39f629b49eaebfc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b122bd9be85a4720b61b11162ec6b51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=9, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fe015896f90>, 'Connection to opihi.cs.uvic.ca timed out. (connect timeout=None)')': /sound/genres.tar.gz\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=8, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fe0158d2000>, 'Connection to opihi.cs.uvic.ca timed out. (connect timeout=None)')': /sound/genres.tar.gz\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=7, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fe0158d2210>, 'Connection to opihi.cs.uvic.ca timed out. (connect timeout=None)')': /sound/genres.tar.gz\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=6, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fe0158d24b0>, 'Connection to opihi.cs.uvic.ca timed out. (connect timeout=None)')': /sound/genres.tar.gz\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=5, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fe0158d2720>, 'Connection to opihi.cs.uvic.ca timed out. (connect timeout=None)')': /sound/genres.tar.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m SELECTED_GENRES = [\u001b[33m\"\u001b[39m\u001b[33mclassical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjazz\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmetal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpop\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrock\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m N_PER_GENRE = \u001b[32m50\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m ds, info = \u001b[43mtfds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtzan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m label_names = info.features[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].names  \u001b[38;5;66;03m# list of 10 genre names\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Map selected genre names to label ids\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/core/logging/__init__.py:176\u001b[39m, in \u001b[36m_FunctionDecorator.__call__\u001b[39m\u001b[34m(self, function, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m._start_call()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    178\u001b[39m   metadata.mark_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/core/load.py:666\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs, file_format)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[32m    542\u001b[39m \n\u001b[32m    543\u001b[39m \u001b[33;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    658\u001b[39m \u001b[33;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n\u001b[32m    660\u001b[39m dbuilder = _fetch_builder(\n\u001b[32m    661\u001b[39m     name=name,\n\u001b[32m    662\u001b[39m     data_dir=data_dir,\n\u001b[32m    663\u001b[39m     builder_kwargs=builder_kwargs,\n\u001b[32m    664\u001b[39m     try_gcs=try_gcs,\n\u001b[32m    665\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    669\u001b[39m   as_dataset_kwargs = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/core/load.py:518\u001b[39m, in \u001b[36m_download_and_prepare_builder\u001b[39m\u001b[34m(dbuilder, download, download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m    517\u001b[39m   download_and_prepare_kwargs = download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m   \u001b[43mdbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/core/logging/__init__.py:176\u001b[39m, in \u001b[36m_FunctionDecorator.__call__\u001b[39m\u001b[34m(self, function, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m._start_call()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    178\u001b[39m   metadata.mark_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:763\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, download_dir, download_config, file_format, permissions)\u001b[39m\n\u001b[32m    761\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.read_from_directory(\u001b[38;5;28mself\u001b[39m.data_dir)\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[32m    769\u001b[39m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[32m    770\u001b[39m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[32m    771\u001b[39m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[32m    772\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.download_size = dl_manager.downloaded_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:1808\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download_config.max_examples_per_split == \u001b[32m0\u001b[39m:\n\u001b[32m   1806\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m split_infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[32m   1811\u001b[39m split_dict = splits_lib.SplitDict(split_infos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:1758\u001b[39m, in \u001b[36mGeneratorBasedBuilder._generate_splits\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1757\u001b[39m   optional_pipeline_kwargs = {}\n\u001b[32m-> \u001b[39m\u001b[32m1758\u001b[39m split_generators = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[32m   1759\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[32m   1764\u001b[39m split_generators = split_builder.normalize_legacy_split_generators(\n\u001b[32m   1765\u001b[39m     split_generators=split_generators,\n\u001b[32m   1766\u001b[39m     generator_fn=\u001b[38;5;28mself\u001b[39m._generate_examples,\n\u001b[32m   1767\u001b[39m     is_beam=\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[32m   1768\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/audio/gtzan/gtzan.py:91\u001b[39m, in \u001b[36mGTZAN._split_generators\u001b[39m\u001b[34m(self, dl_manager)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[32m     90\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns SplitGenerators.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m   dl_paths = \u001b[43mdl_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenres\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_DOWNLOAD_URL\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m   path = os.path.join(dl_paths[\u001b[33m\"\u001b[39m\u001b[33mgenres\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mgenres\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m   \u001b[38;5;66;03m# There is no predefined train/val/test split for this dataset.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:754\u001b[39m, in \u001b[36mDownloadManager.download_and_extract\u001b[39m\u001b[34m(self, url_or_urls)\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._downloader.tqdm():\n\u001b[32m    753\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extractor.tqdm():\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:782\u001b[39m, in \u001b[36m_map_promise\u001b[39m\u001b[34m(map_fn, all_inputs)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m all_promises = tree.map_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m res = \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tree/__init__.py:428\u001b[39m, in \u001b[36mmap_structure\u001b[39m\u001b[34m(func, *structures, **kwargs)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[32m1\u001b[39m:]:\n\u001b[32m    426\u001b[39m   assert_same_structure(structures[\u001b[32m0\u001b[39m], other, check_types=check_types)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[32m0\u001b[39m],\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:782\u001b[39m, in \u001b[36m_map_promise.<locals>.<lambda>\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m all_promises = tree.map_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m res = tree.map_structure(\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises)  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/promise/promise.py:511\u001b[39m, in \u001b[36mPromise.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    509\u001b[39m     \u001b[38;5;66;03m# type: (Optional[float]) -> T\u001b[39;00m\n\u001b[32m    510\u001b[39m     target = \u001b[38;5;28mself\u001b[39m._target()\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEFAULT_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._target_settled_value(_raise=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/promise/promise.py:506\u001b[39m, in \u001b[36mPromise._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    505\u001b[39m     \u001b[38;5;66;03m# type: (Optional[float]) -> None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/promise/promise.py:502\u001b[39m, in \u001b[36mPromise.wait\u001b[39m\u001b[34m(cls, promise, timeout)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait\u001b[39m(\u001b[38;5;28mcls\u001b[39m, promise, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    501\u001b[39m     \u001b[38;5;66;03m# type: (Promise, Optional[float]) -> None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     \u001b[43masync_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpromise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/promise/async_.py:117\u001b[39m, in \u001b[36mAsync.wait\u001b[39m\u001b[34m(self, promise, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m promise.is_pending:\n\u001b[32m    114\u001b[39m         \u001b[38;5;66;03m# We return if the promise is already\u001b[39;00m\n\u001b[32m    115\u001b[39m         \u001b[38;5;66;03m# fulfilled or rejected\u001b[39;00m\n\u001b[32m    116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/promise/schedulers/immediate.py:25\u001b[39m, in \u001b[36mImmediateScheduler.wait\u001b[39m\u001b[34m(self, promise, timeout)\u001b[39m\n\u001b[32m     22\u001b[39m     e.set()\n\u001b[32m     24\u001b[39m promise._then(on_resolve_or_reject, on_resolve_or_reject)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m waited = \u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m waited:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTimeout\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fe0158d28a0>, 'Connection to opihi.cs.uvic.ca timed out. (connect timeout=None)')': /sound/genres.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Choose the genres you want (names must match TFDS label names)\n",
    "SELECTED_GENRES = [\"classical\", \"jazz\", \"metal\", \"pop\", \"rock\"]\n",
    "N_PER_GENRE = 50\n",
    "\n",
    "ds, info = tfds.load(\"gtzan\", split=\"train\", with_info=True, as_supervised=True)\n",
    "label_names = info.features[\"label\"].names  # list of 10 genre names\n",
    "\n",
    "# Map selected genre names to label ids\n",
    "selected_label_ids = tf.constant([label_names.index(g) for g in SELECTED_GENRES], dtype=tf.int64)\n",
    "\n",
    "def keep_selected(audio, label):\n",
    "    # label is scalar int\n",
    "    return tf.reduce_any(tf.equal(label, selected_label_ids))\n",
    "\n",
    "filtered = ds.filter(keep_selected)\n",
    "\n",
    "# Build a balanced subset: exactly N_PER_GENRE examples per selected label\n",
    "def balanced_take(ds_in, label_ids, n_per_label):\n",
    "    per_label = []\n",
    "    for lid in label_ids.numpy().tolist():\n",
    "        ds_l = ds_in.filter(lambda a, y, lid=lid: tf.equal(y, tf.cast(lid, tf.int64))).take(n_per_label)\n",
    "        per_label.append(ds_l)\n",
    "    # Concatenate and shuffle\n",
    "    out = per_label[0]\n",
    "    for d in per_label[1:]:\n",
    "        out = out.concatenate(d)\n",
    "    return out.shuffle(len(SELECTED_GENRES) * n_per_label, reshuffle_each_iteration=False)\n",
    "\n",
    "subset = balanced_take(filtered, selected_label_ids, N_PER_GENRE)\n",
    "\n",
    "print(\"Subset size:\", tf.data.experimental.cardinality(subset).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b1bf2-584c-4275-b543-d74fa2b3d468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
